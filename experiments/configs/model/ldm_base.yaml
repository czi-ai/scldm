defaults:
  - vae_base # this is actually not used cause it's imported from checkpoint

module:
  _target_: scldm.models.LatentDiffusion
  vae_as_tokenizer:
    train: false
    load_from_checkpoint:
      ckpt_path: null  # Set in ldm_training.yaml
      job_name: null
      epoch: null
  diffusion_model:
    # _target_: scldm.diffusion.FlowMatching
    # nnet:
    _target_: scldm.nnets.DiT
    n_embed: 256
    n_embed_input: ${model.module.vae_model.encoder.n_embed_latent}
    seq_len: ${model.module.vae_model.encoder.n_inducing_points}
    n_layer: 8
    n_head: 8
    dropout: 0.0
    bias: true
    norm_layer: layernorm
    multiple_of: 4
    layernorm_eps: ${model.module.vae_model.encoder.layernorm_eps}
    class_vocab_sizes: ${datamodule.vocabulary_encoder.class_vocab_sizes}
    cfg_dropout_prob: 0.8
    condition_strategy: mutually_exclusive
  transport:
    _target_: scldm.transport.create_transport
    path_type: Linear
    prediction: velocity
    loss_weight: velocity
    train_eps: 1e-5
    sample_eps: 1e-5
  diffusion_optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 5e-4
    weight_decay: 0.0
    # betas: [0.9, 0.95]
    #betas: [0.9, 0.95]
  diffusion_scheduler:
    _target_: scldm._utils.wsd_schedule
    num_training_steps: ${training.trainer.max_steps}
    final_lr_factor: 0.1
    num_warmup_steps: null
    init_div_factor: 100
    fract_decay: 1.0
    decay_type: cosine
  ema_decay: 0.9999
  ema_update_every: 10
  update_after_step: 10_000
  allow_different_devices: true
  use_foreach: true
  calculate_grad_norms: false

batch_size: 128
test_batch_size: 128

num_parameters: null
flops: null

get_flops:
  _target_: scldm.flops.get_flops
  seq_len: ${model.module.diffusion_model.seq_len}
  vocab_size: ${model.module.diffusion_model.seq_len}
  num_heads: ${model.module.diffusion_model.n_head}
  swiglu: false
  n_layers: ${model.module.diffusion_model.n_layer}
  d_model: ${model.module.diffusion_model.n_embed}
  key_size: ${model.module.diffusion_model.n_embed}
  ffw_size: ${eval:'${model.module.diffusion_model.n_embed} * ${model.module.diffusion_model.multiple_of}'}
